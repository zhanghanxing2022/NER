{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ChainCRF\n",
    "import importlib\n",
    "import dataLoad.lload\n",
    "importlib.reload(model.ChainCRF)\n",
    "importlib.reload(dataLoad.lload)\n",
    "from  dataLoad.lload import CustomDataset\n",
    "from  dataLoad.lload import *\n",
    "\n",
    "from model.ChainCRF import BLITM\n",
    "import torchtext\n",
    "from get_data import *\n",
    "from check import *\n",
    "from collections import Counter,defaultdict\n",
    "embed_size = 100\n",
    "language = \"Chinese\"\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123967\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = get_train_data(language)\n",
    "import os\n",
    "import torchtext\n",
    "train_word = [word  for sentence in train_data for word,label in sentence]\n",
    "print(len(train_word))\n",
    "vocab = torchtext.vocab.vocab(Counter(train_word),min_freq=min_freq,specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "print(vocab.get_default_index())\n",
    "torch.save(vocab.state_dict(),\"./bilstm_crf/pretrain/{}.pt\".format(language))\n",
    "def sent2word(sentence):\n",
    "    return [w for w, _ in sentence]\n",
    "def sent2label(sentence):\n",
    "    return [l for _, l in sentence]\n",
    "max_length = max([len(l) for l in train_data])\n",
    "max_length = max(max_length, 256)\n",
    "sorted_labels = sorted_labels_chn if language == 'Chinese' else sorted_labels_eng\n",
    "def label2index(label):\n",
    "    return sorted_labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_embeddings': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]),\n",
       " 'label_indices': tensor([ 0,  0, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 13, 14, 14, 14, 14, 15,\n",
       "          0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'max_length': 19}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(train_data, vocab, label2index, max_length)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "def train(model, train_loader, num_epochs, learning_rate, device):\n",
    "    # Move the model to GPU\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # ignore padding index\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=num_epochs * len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move inputs, labels, and mask to GPU\n",
    "            input_ = batch['word_embeddings']\n",
    "            label_ = batch['label_indices']\n",
    "            mask_ = batch['mask'].to(device)\n",
    "            length = batch['max_length']\n",
    "            max_length = np.argmax(length)\n",
    "            aaa = length[max_length]\n",
    "            inputs =torch.Tensor(input_[:,:aaa])\n",
    "            labels =torch.Tensor( label_[:,:aaa])\n",
    "            mask = torch.Tensor(mask_[:,:aaa])\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            # loss = model(input_, mask_, label_)\n",
    "            outputs = model(inputs)\n",
    "            xx = outputs.shape[-1]\n",
    "            outputs = outputs.view(-1,xx)\n",
    "            labels = labels.reshape(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Mask out the padding positions\n",
    "            # loss = (loss * mask.reshape(-1)).sum() / mask.sum()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # Update the parameters\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.set_postfix_str(\"Epoch:{}, Loss:{}\".format(epoch + 1, sum_loss / len(train_loader)))\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, train_loader, num_epochs, and learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:16<00:00, 15.62it/s, Epoch:10, Loss:0.06934428222787876]\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "bilstm = BLITM(len(sorted_labels),len(vocab),embed_size,hidden_dim)\n",
    "file = \"./bilstm_crf/pretrain/BILSTM_{}.bin\".format(language)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mode = True\n",
    "if mode:\n",
    "    train(bilstm,dataloader,10,1e-2,device)\n",
    "    torch.save(bilstm.state_dict(),file)\n",
    "else:\n",
    "    bilstm.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycheck(language, vocab, res_file, model, train_or_valid,device):\n",
    "    valid = get_data_from_file(res_file)\n",
    "    pred_path = \"example_data/BILSTM_{}_{}.txt\".format(language, \"train\" if train_or_valid else \"dev\")\n",
    "    valid_data = CustomDataset(valid, vocab, label2index, 250)\n",
    "    valdataloader = DataLoader(valid_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Move the model to GPU\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    with open(pred_path, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            iter = 0\n",
    "            for batch in valdataloader:\n",
    "                max_length = np.argmax(batch['max_length'])\n",
    "                aaa = batch['max_length'][max_length]\n",
    "                word_embeddings = batch['word_embeddings'][:,:aaa]\n",
    "\n",
    "                preds = model(word_embeddings)\n",
    "                _,preds= torch.max(preds,dim=-1)\n",
    "\n",
    "                for pred in preds:\n",
    "                    pred_labels = []\n",
    "                    for i in range(len(valid[iter])):\n",
    "                        f.write(valid[iter][i][0] + \" \" + sorted_labels[pred[i]] + '\\n')\n",
    "                        pred_labels.append(sorted_labels[pred[i]])\n",
    "                    f.write('\\n')\n",
    "                    iter = iter + 1\n",
    "\n",
    "    # Move the model back to CPU if needed\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    check(language, \"{}/{}.txt\".format(language, \"train\" if train_or_valid else \"validation\"), pred_path)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, vocab, res_file, max_length, train_or_valid, label2index, sorted_labels\n",
    "# Make sure to replace the imports with the actual modules and functions in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-NAME     0.9400    0.9216    0.9307       102\n",
      "      M-NAME     0.8072    0.8933    0.8481        75\n",
      "      E-NAME     0.9798    0.9510    0.9652       102\n",
      "      S-NAME     0.7778    0.8750    0.8235         8\n",
      "      B-CONT     1.0000    0.9394    0.9688        33\n",
      "      M-CONT     1.0000    0.9375    0.9677        64\n",
      "      E-CONT     1.0000    1.0000    1.0000        33\n",
      "      S-CONT     0.0000    0.0000    0.0000         0\n",
      "       B-EDU     0.9533    0.9623    0.9577       106\n",
      "       M-EDU     0.9778    0.9944    0.9860       177\n",
      "       E-EDU     0.9626    0.9717    0.9671       106\n",
      "       S-EDU     0.0000    0.0000    0.0000         0\n",
      "     B-TITLE     0.9020    0.9086    0.9053       689\n",
      "     M-TITLE     0.8815    0.9304    0.9053      1479\n",
      "     E-TITLE     0.9897    0.9797    0.9847       689\n",
      "     S-TITLE     0.0000    0.0000    0.0000         0\n",
      "       B-ORG     0.9551    0.8966    0.9249       522\n",
      "       M-ORG     0.9571    0.9428    0.9499      3622\n",
      "       E-ORG     0.8878    0.8640    0.8757       522\n",
      "       S-ORG     0.0000    0.0000    0.0000         0\n",
      "      B-RACE     1.0000    1.0000    1.0000        14\n",
      "      M-RACE     0.0000    0.0000    0.0000         0\n",
      "      E-RACE     1.0000    1.0000    1.0000        14\n",
      "      S-RACE     0.0000    0.0000    0.0000         1\n",
      "       B-PRO     0.7727    0.9444    0.8500        18\n",
      "       M-PRO     0.7838    0.8788    0.8286        33\n",
      "       E-PRO     0.8947    0.9444    0.9189        18\n",
      "       S-PRO     0.0000    0.0000    0.0000         0\n",
      "       B-LOC     1.0000    1.0000    1.0000         2\n",
      "       M-LOC     0.7500    1.0000    0.8571         6\n",
      "       E-LOC     1.0000    1.0000    1.0000         2\n",
      "       S-LOC     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.9350    0.9342    0.9346      8437\n",
      "   macro avg     0.6929    0.7105    0.7005      8437\n",
      "weighted avg     0.9360    0.9342    0.9348      8437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# mycheck(language=language,vocab=vocab,res_file=\"{}/train.txt\".format(language),model=bilstm_crf,train_or_valid=1,device=device)\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/validation.txt\".format(language),model=bilstm,train_or_valid=0,device=device)\n",
    "# loss:114 0.3803  0.3954\n",
    "# -92.54755198711143 0.3460 0.3643"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
