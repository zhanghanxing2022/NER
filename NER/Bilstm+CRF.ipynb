{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3706709772.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i in b\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# 在BiLSTM+CRF模型中，BiLSTM部分可以使用Pytorch等深度学习框架，CRF部分必须手写完成。\n",
    "# https://github.com/phipleg/keras/blob/crf/keras/layers/crf.py\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size,embed_matrix, hidden_size, tagset_size, num_layers=1, dropout=0.1):\n",
    "        super(Model, self).__init__()\n",
    "        if embed_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(embed_matrix, dtype=torch.float), freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.bilstm = nn.LSTM(embed_size, hidden_size//2,\n",
    "                              num_layers, dropout=dropout, bidirectional=True)\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_size, tagset_size)\n",
    "        self.crf_transition = nn.Parameter(\n",
    "            torch.rand((tagset_size, tagset_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        lstmout, _ = self.bilstm(embed)\n",
    "        emissions = self.hidden2tag(lstmout)\n",
    "        #   seq_length,  batch_size , output_size\n",
    "        seq_length,  batch_size, output_size = emissions.shape\n",
    "        score = torch.zeros(emissions.shape)\n",
    "        path = torch.zeros(emissions.shape)\n",
    "        for i in batch_size:\n",
    "            for j in seq_length:\n",
    "                for k in range(output_size):\n",
    "                    if j == 0:\n",
    "                        score[j][i][k] = emissions[j][i][k]\n",
    "                        path[j][i][k] = k\n",
    "                    else:\n",
    "                        max_score = float(\"-inf\")\n",
    "                        max_prev = 0\n",
    "                        for l in range(output_size):\n",
    "                            temp_score = score[j-1][i][l] + \\\n",
    "                                emissions[j][i][k]+self.crf_transition[l][k]\n",
    "                            if temp_score > max_score:\n",
    "                                max_score = temp_score\n",
    "                                max_prev = l\n",
    "                        score[j][i][k] = max_score\n",
    "                        path[j][i][k] = max_prev\n",
    "        return score, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ChainCRF(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChainCRF, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 定义转移矩阵 U、起始边界 b_start、结束边界 b_end\n",
    "        self.U = nn.Parameter(torch.rand(num_classes, num_classes))\n",
    "        self.b_start = nn.Parameter(torch.zeros(num_classes))\n",
    "        self.b_end = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, emissions, true_tags):\n",
    "        \"\"\"\n",
    "        :param emissions: [batch_size, seq_len, num_classes] - emission scores\n",
    "        :param true_tags: [batch_size, seq_len] - true tag indices\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        # Calculate negative log-likelihood loss\n",
    "        path_energy = self.path_energy(emissions, true_tags)\n",
    "        free_energy = self.free_energy(emissions)\n",
    "        loss = torch.mean(path_energy - free_energy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def path_energy(self, emissions, true_tags):\n",
    "        \"\"\"\n",
    "        : return [batch_size]\n",
    "        \"\"\"\n",
    "        true_tags_mask = torch.one_hot(true_tags, self.num_classes)\n",
    "        energy = true_tags_mask*emissions\n",
    "        energy = torch.sum(energy, dim=2)\n",
    "        prev = true_tags[:, :-1]\n",
    "        next = true_tags[:, 1:]\n",
    "        transitions = prev*self.num_classes+next\n",
    "        U_flat = self.U.reshape((-1))\n",
    "        U_y_t_em = U_flat[transitions]\n",
    "        energy += torch.sum(U_y_t_em, dim=1)\n",
    "        return energy\n",
    "\n",
    "    def free_energy(self, emissions):\n",
    "        \"\"\"Compute the free energy using the forward algorithm.\n",
    "        :param emissions: [batch_size, seq_len, num_classes] - emission scores\n",
    "        :return: [batch_size] - free energy for each sequence in the batch\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = emissions.size()\n",
    "\n",
    "        # Initialize the alpha values with the emission scores at the first position\n",
    "        alpha = emissions[:, 0, :]\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            # Calculate the energy of each possible transition\n",
    "            # alpha batch_size,num_classes,1\n",
    "            # U     1         ,num_classes,num_classes\n",
    "            transition_energy = alpha.unsqueeze(2) + self.U.unsqueeze(0)\n",
    "\n",
    "            # Sum over the previous state (dim=1) to get the new alpha values\n",
    "            alpha = torch.logsumexp(\n",
    "                transition_energy, dim=1) + emissions[:, t, :]\n",
    "\n",
    "        # Add the boundary energies\n",
    "        alpha += self.b_end.unsqueeze(0)\n",
    "\n",
    "        # Sum over the last state (dim=1) to get the free energy\n",
    "        free_energy = -torch.logsumexp(alpha, dim=1)\n",
    "        return free_energy\n",
    "\n",
    "    def viterbi_decode(self, emission):\n",
    "        \"\"\"Decode the highest scoring sequence of tags using the Viterbi algorithm.\n",
    "        :param emission: [batch_size, seq_len, num_classes] - emission scores\n",
    "        :return: [batch_size, seq_len] - the tag indices of the highest scoring sequence\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_classes = emission.size()\n",
    "\n",
    "        score = torch.zeros(emission.shape)\n",
    "        path = torch.zeros(emission.shape, dtype=torch.long)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                for k in range(num_classes):\n",
    "                    if j == 0:\n",
    "                        score[i, j, k] = emission[i, j, k]\n",
    "                        path[i, j, k] = k\n",
    "                    else:\n",
    "                        max_score = float(\"-inf\")\n",
    "                        max_prev = 0\n",
    "                        for l in range(num_classes):\n",
    "                            temp_score = score[i, j-1, l] + \\\n",
    "                                emission[i, j, k] + self.U[l, k]\n",
    "                            if temp_score > max_score:\n",
    "                                max_score = temp_score\n",
    "                                max_prev = l\n",
    "                        score[i, j, k] = max_score\n",
    "                        path[i, j, k] = max_prev\n",
    "\n",
    "        # Backtrack to find the best path\n",
    "        best_path = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "        _, best_last_tag = torch.max(score[:, -1, :], dim=1)\n",
    "        best_path[:, -1] = best_last_tag\n",
    "\n",
    "        for j in range(seq_len - 2, -1, -1):\n",
    "            for i in range(batch_size):\n",
    "                best_path[i, j] = path[i, j + 1, best_path[i, j + 1]]\n",
    "\n",
    "        return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # 定义CRF层\n",
    "        self.crf = ChainCRF(num_classes)\n",
    "\n",
    "    def forward(self, sentence, targets=None):\n",
    "        # 获取词嵌入\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "\n",
    "        # BiLSTM层\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 计算CRF损失\n",
    "            # mask = (sentence != 0)  # 使用 0 填充的词的位置作为掩码\n",
    "            crf_loss = self.crf(emissions, targets)\n",
    "            return crf_loss\n",
    "        else:\n",
    "            # 测试时，使用维特比解码\n",
    "            tags = self.crf.viterbi_decode(emissions)\n",
    "            return tags\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "num_classes = 5\n",
    "embedding_dim = 50\n",
    "hidden_dim = 50\n",
    "vocab_size = 10000\n",
    "model = BiLSTM_CRF(num_classes, embedding_dim, hidden_dim, vocab_size)\n",
    "\n",
    "# 示例输入\n",
    "sentence = torch.randint(1, vocab_size, (2, 10))  # 2个句子，每个句子有10个词\n",
    "\n",
    "# 训练\n",
    "targets = torch.tensor([[1, 2, 3, 4, 0, 0, 0, 0, 0, 0],\n",
    "                       [3, 1, 4, 0, 0, 0, 0, 0, 0, 0]])\n",
    "loss = model(sentence, targets)\n",
    "print(\"训练损失:\", loss.item())\n",
    "\n",
    "# 测试\n",
    "tags = model(sentence)\n",
    "print(\"预测标签:\", tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChainCRF(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ChainCRF, self).__init__()\n",
    "        self.U = nn.Parameter(torch.rand(num_classes, num_classes))\n",
    "        self.b_start = nn.Parameter(torch.zeros(num_classes))\n",
    "        self.b_end = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def path_energy(self, y, x, mask=None):\n",
    "        x = self.add_boundary_energy(x, mask)\n",
    "        return self.path_energy0(y, x, mask)\n",
    "\n",
    "    def path_energy0(self, y, x, mask=None):\n",
    "        n_classes = x.size(2)\n",
    "        y_one_hot = F.one_hot(y, n_classes).float()\n",
    "\n",
    "        # Tag path energy\n",
    "        energy = torch.sum(x * y_one_hot, dim=2)\n",
    "        energy = torch.sum(energy, dim=1)\n",
    "\n",
    "        # Transition energy\n",
    "        y_t = y[:, :-1]\n",
    "        y_tp1 = y[:, 1:]\n",
    "        U_flat = self.U.view(-1)\n",
    "        flat_indices = y_t * n_classes + y_tp1\n",
    "        U_y_t_tp1 = U_flat[flat_indices]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.float()\n",
    "            y_t_mask = mask[:, :-1]\n",
    "            y_tp1_mask = mask[:, 1:]\n",
    "            U_y_t_tp1 *= y_t_mask * y_tp1_mask\n",
    "\n",
    "        energy += torch.sum(U_y_t_tp1, dim=1)\n",
    "\n",
    "        return energy\n",
    "\n",
    "    def sparse_chain_crf_loss(self, y, x, mask=None):\n",
    "        x = self.add_boundary_energy(x, mask)\n",
    "        energy = self.path_energy0(y, x, mask)\n",
    "        energy -= self.free_energy0(x, mask)\n",
    "        return -energy.unsqueeze(-1)\n",
    "\n",
    "    def add_boundary_energy(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            x = torch.cat([x[:, :1, :] + self.b_start, x[:, 1:, :]], dim=1)\n",
    "            x = torch.cat([x[:, :-1, :], x[:, -1:, :] + self.b_end], dim=1)\n",
    "        else:\n",
    "            mask = mask.float()\n",
    "            x *= mask\n",
    "            start_mask = torch.cat([torch.zeros_like(mask[:, :1]), mask[:, :-1]], dim=1)\n",
    "            start_mask = (mask > start_mask).float()\n",
    "            x = x + start_mask.unsqueeze(-1) * self.b_start\n",
    "            end_mask = torch.cat([mask[:, 1:], torch.zeros_like(mask[:, -1:])], dim=1)\n",
    "            end_mask = (mask > end_mask).float()\n",
    "            x = x + end_mask.unsqueeze(-1) * self.b_end\n",
    "        return x\n",
    "\n",
    "    def viterbi_decode(self, x, mask=None):\n",
    "        x = self.add_boundary_energy(x, mask)\n",
    "\n",
    "        alpha_0 = x[:, 0, :]\n",
    "        gamma_0 = torch.zeros_like(alpha_0)\n",
    "        initial_states = [gamma_0, alpha_0]\n",
    "        _, gamma = self._forward(x, initial_states, mask)\n",
    "        y = self._backward(gamma, mask)\n",
    "        return y\n",
    "\n",
    "    def free_energy(self, x, mask=None):\n",
    "        x = self.add_boundary_energy(x, mask)\n",
    "        return self.free_energy0(x, mask)\n",
    "\n",
    "    def free_energy0(self, x, mask=None):\n",
    "        initial_states = [x[:, 0, :]]\n",
    "        last_alpha, _ = self._forward(x, initial_states, mask)\n",
    "        return last_alpha[:, 0]\n",
    "\n",
    "    def _forward(self, x, states, mask=None):\n",
    "        def _forward_step(energy_matrix_t, states):\n",
    "            alpha_tm1 = states[-1]\n",
    "            new_states = [torch.logsumexp(alpha_tm1.unsqueeze(2) + energy_matrix_t, dim=1)]\n",
    "            return new_states[0], new_states\n",
    "\n",
    "        U_shared = self.U.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.float()\n",
    "            mask_U = (mask[:, :-1] * mask[:, 1:]).unsqueeze(2).unsqueeze(3)\n",
    "            U_shared = U_shared * mask_U\n",
    "\n",
    "        inputs = (x[:, 1:, :] + U_shared).contiguous()\n",
    "        inputs = torch.cat([inputs, torch.zeros_like(inputs[:, -1:, :, :])], dim=1)\n",
    "\n",
    "        last, values = self._rnn(_forward_step, inputs, states)\n",
    "        return last, values\n",
    "\n",
    "    def _rnn(self, fn, inputs, initial_states):\n",
    "        def step(input, states):\n",
    "            return fn(input, states)\n",
    "\n",
    "        return torch.scan(step, inputs, initial_states)\n",
    "\n",
    "    def _backward(self, gamma, mask):\n",
    "        gamma = gamma.int()\n",
    "\n",
    "        def _backward_step(gamma_t, states):\n",
    "            y_tm1 = states[0].squeeze(0)\n",
    "            y_t = gamma_t.gather(1, y_tm1.unsqueeze(1))\n",
    "            return y_t, [y_t.unsqueeze(0)]\n",
    "\n",
    "        initial_states = [torch.zeros_like(gamma[:, 0, 0]).unsqueeze(0)]\n",
    "        _, y_rev = self._rnn(_backward_step, gamma, initial_states)\n",
    "        y = torch.flip(y_rev, [1])\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.int()\n",
    "            y *= mask\n",
    "            y += -(1 - mask)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # During training, return x; during testing, return viterbi decoding\n",
    "        return F.in_train_mode(x, self.viterbi_decode(x))\n",
    "\n",
    "# Example Usage\n",
    "num_classes = 5\n",
    "crf = ChainCRF(num_classes)\n",
    "x = torch.rand(2, 4, num_classes)  # Batch size of 2, sequence length of 4\n",
    "y_true = torch.tensor([[1, 2, 3, 4], [0, 2, 1, 3]])  # Example true tag sequences\n",
    "\n",
    "# Training\n",
    "loss = crf.sparse_chain_crf_loss(y_true, x)\n",
    "print(\"Training Loss:\", loss.item())\n",
    "\n",
    "# Testing\n",
    "y_pred = crf(x)\n",
    "print(\"Predicted Tags:\", y_pred.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
