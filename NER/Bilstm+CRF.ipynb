{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# 在BiLSTM+CRF模型中，BiLSTM部分可以使用Pytorch等深度学习框架，CRF部分必须手写完成。\n",
    "# https://github.com/phipleg/keras/blob/crf/keras/layers/crf.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ChainCRF\n",
    "import importlib\n",
    "importlib.reload(model.ChainCRF)\n",
    "from model.ChainCRF import ChainCRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        # self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # 词嵌入层\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # 定义CRF层\n",
    "        self.crf = ChainCRF(num_classes)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence, mask,targets=None):\n",
    "        # 获取词嵌入\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        embeds = sentence\n",
    "        # BiLSTM层\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # [batch_size,max_length,hidden_dim]\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        # print(lstm_out.shape)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        # emissions = self.sigmoid(emissions)\n",
    "\n",
    "        # print(emissions.shape)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 计算CRF损失\n",
    "            # mask = (sentence != 0)  # 使用 0 填充的词的位置作为掩码\n",
    "            crf_loss = self.crf(emissions,targets,mask)\n",
    "            return crf_loss\n",
    "        else:\n",
    "            # 测试时，使用维特比解码\n",
    "            tags = self.crf.viterbi_decode(emissions,mask)\n",
    "            return tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n",
      "tensor([ 5.6719e-02,  1.3333e-01,  7.2690e-01, -4.6336e-01, -5.9334e-01,\n",
      "         7.1746e-01, -1.1795e-01,  2.1614e-01,  4.3036e-01, -6.7053e-01,\n",
      "         5.7480e-01,  2.6827e-01,  2.4659e-02,  1.6066e-01,  2.0400e-01,\n",
      "        -3.9246e-01, -6.3294e-01,  6.2915e-01, -7.6340e-01,  1.1581e+00,\n",
      "         3.6218e-01,  3.1932e-01, -6.5613e-01, -4.7797e-01,  2.9885e-01,\n",
      "         6.2435e-01, -4.6060e-01, -9.6276e-01,  1.2214e+00, -2.3152e-01,\n",
      "        -6.8889e-02,  6.3519e-01,  7.7546e-01,  3.3128e-01, -3.5220e-01,\n",
      "         7.4236e-01, -6.6703e-01,  3.2260e-01,  4.3490e-01, -6.0154e-01,\n",
      "        -4.2067e-01,  2.1991e-02,  1.6378e-01, -9.5682e-01, -6.4464e-01,\n",
      "        -9.4111e-02, -2.7105e-01, -2.3312e-01, -3.8453e-01, -1.2665e+00,\n",
      "        -1.8289e-01,  5.0432e-01, -5.4260e-02,  1.1872e+00, -4.7143e-01,\n",
      "        -2.6562e+00,  4.4917e-01,  6.7218e-01,  1.4074e+00,  1.9179e-03,\n",
      "         4.0658e-01,  1.4287e+00, -1.0631e+00, -2.1713e-01,  4.7800e-01,\n",
      "         1.3170e-01,  1.2494e+00,  7.2980e-01, -2.0880e-01,  2.5449e-01,\n",
      "         1.2297e-01,  3.4922e-01,  2.4051e-01, -8.1023e-01,  5.2047e-01,\n",
      "         6.8801e-01,  6.7784e-02, -2.2132e-01, -1.2176e-01, -1.6238e-01,\n",
      "         5.3189e-01, -3.2943e-01, -5.3818e-01, -2.2957e-01, -1.4103e+00,\n",
      "         1.6494e-02, -1.3494e-01, -3.6251e-02, -7.7385e-01, -3.5178e-01,\n",
      "        -1.1230e-01, -3.7400e-01,  5.5139e-01, -1.9572e-01, -8.7050e-02,\n",
      "        -3.1469e-01, -4.2257e-01, -3.5286e-02, -2.4911e-02,  6.2131e-01])\n",
      "14041\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from check import *\n",
    "embed_size = 100\n",
    "glove_path=\"glove/\"\n",
    "vocab = torchtext.vocab.GloVe(name=\"6B\",dim=embed_size,cache=glove_path)\n",
    "print(vocab['中'])\n",
    "print(vocab['me'])\n",
    "from get_data import *\n",
    "language ='English'\n",
    "train_data = get_train_data(language)\n",
    "def sent2word(sentence):\n",
    "    return [w for w, _ in sentence]\n",
    "def sent2label(sentence):\n",
    "    return [l for _, l in sentence]\n",
    "max_length = max([len(l) for l in train_data])\n",
    "max_length = max(max_length, 128)\n",
    "sorted_labels = sorted_labels_chn if language == 'Chinese' else sorted_labels_eng\n",
    "def label2index(label):\n",
    "    return sorted_labels.index(label)\n",
    "#对句子的word进行词嵌入，对tag转换为index，对句子按照max_length进行截断或者填充，并产生对应的mask，最后产生对应的dataset\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Assuming train_data is a list of sentences where each sentence is a list of tuples (word, label)\n",
    "# Example: [('This', 'O'), ('is', 'O'), ('a', 'O'), ('sentence', 'B')]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, label2index, max_length):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.label2index = label2index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "\n",
    "        # Extract words and labels\n",
    "        words = [w for w, _ in sentence]\n",
    "        labels = [l for _, l in sentence]\n",
    "\n",
    "        # Convert words to embeddings\n",
    "        word_embeddings = [self.vocab[w] for w in words]\n",
    "\n",
    "        # Convert labels to indices\n",
    "        label_indices = [self.label2index(l) for l in labels]\n",
    "\n",
    "        # Pad or truncate to max_length\n",
    "        if len(word_embeddings) < self.max_length:\n",
    "            pad_length = self.max_length - len(word_embeddings)\n",
    "            word_embeddings = word_embeddings + [torch.zeros_like(word_embeddings[0])] * pad_length\n",
    "            label_indices = label_indices + [0] * pad_length  # Assuming 0 is the index for padding\n",
    "        else:\n",
    "            word_embeddings = word_embeddings[:self.max_length]\n",
    "            label_indices = label_indices[:self.max_length]\n",
    "\n",
    "        # Create a mask\n",
    "        mask = [1] * min(len(words),max_length) + [0] * max(0,(self.max_length - len(words)))\n",
    "\n",
    "        return {\n",
    "            'word_embeddings': torch.stack(word_embeddings),\n",
    "            'label_indices': torch.tensor(label_indices),\n",
    "            'mask': torch.tensor(mask)\n",
    "        }\n",
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(train_data, vocab, label2index, max_length)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, num_epochs, learning_rate, device):\n",
    "    # Move the model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=num_epochs * len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move inputs, labels, and mask to GPU\n",
    "            inputs = batch['word_embeddings'].to(device)\n",
    "            labels = batch['label_indices'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(inputs, mask, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.set_postfix_str(\"Epoch:{}, Loss:{}\".format(epoch + 1, sum_loss / len(train_loader)))\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, train_loader, num_epochs, and learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m mode:\n\u001b[0;32m----> 8\u001b[0m     train(bilstm_crf,dataloader,\u001b[39m10\u001b[39;49m,\u001b[39m1e-2\u001b[39;49m,device)\n\u001b[1;32m      9\u001b[0m     torch\u001b[39m.\u001b[39msave(bilstm_crf\u001b[39m.\u001b[39mstate_dict(),file)\n\u001b[1;32m     10\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m model(inputs, mask, labels)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     33\u001b[0m sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m \u001b[39m# Update the parameters\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "bilstm_crf = BiLSTM_CRF(len(sorted_labels),embed_size,hidden_dim)\n",
    "file = \"BILSTM_CRF_{}.bin\".format(language)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "mode = True\n",
    "if mode:\n",
    "    train(bilstm_crf,dataloader,10,1e-2,device)\n",
    "    torch.save(bilstm_crf.state_dict(),file)\n",
    "else:\n",
    "    bilstm_crf.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycheck(language,vocab,res_file,model,max_length,train_or_valid):\n",
    "    valid = get_data_from_file(res_file)\n",
    "    pred_path = \"example_data/BILSTM_CRF_{}_{}.txt\".format(language,\"train\" if train_or_valid else \"dev\")\n",
    "    valid_data = CustomDataset(valid, vocab, label2index, max_length)\n",
    "    valdataloader = DataLoader(valid_data, batch_size=64, shuffle=False)\n",
    "    iter = 0\n",
    "    with open(pred_path, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            iter = 0\n",
    "            for val in valdataloader:\n",
    "                preds = model(val['word_embeddings'],val['mask'])\n",
    "                masks = val['mask']\n",
    "                for pred,mask in zip(preds,masks):\n",
    "                    pred_labels = []\n",
    "                    for i in range(len(pred)):\n",
    "                        if mask[i] == 1:\n",
    "                            f.write(valid[iter][i][0] +\" \" +sorted_labels[pred[i]]+'\\n')\n",
    "                            pred_labels.append(sorted_labels[pred[i]])\n",
    "                        else:\n",
    "                            f.write('\\n')\n",
    "                            iter = iter+1\n",
    "                            break\n",
    "    check(language,\"{}/{}.txt\".format(language,\"train\" if train_or_valid else \"validation\"),pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.5397    0.5052    0.5218      6600\n",
      "       I-PER     0.5383    0.7365    0.6220      4528\n",
      "       B-ORG     0.2627    0.4700    0.3371      6321\n",
      "       I-ORG     0.3380    0.5715    0.4248      3704\n",
      "       B-LOC     0.4574    0.2083    0.2862      7140\n",
      "       I-LOC     0.0790    0.0199    0.0318      1157\n",
      "      B-MISC     0.1072    0.0492    0.0674      3438\n",
      "      I-MISC     0.1613    0.0130    0.0240      1155\n",
      "\n",
      "   micro avg     0.3826    0.3951    0.3888     34043\n",
      "   macro avg     0.3105    0.3217    0.2894     34043\n",
      "weighted avg     0.3767    0.3951    0.3614     34043\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.6063    0.5510    0.5774      1842\n",
      "       I-PER     0.6033    0.7796    0.6802      1307\n",
      "       B-ORG     0.2156    0.4452    0.2905      1341\n",
      "       I-ORG     0.2832    0.5087    0.3638       751\n",
      "       B-LOC     0.4542    0.1943    0.2722      1837\n",
      "       I-LOC     0.1429    0.0272    0.0458       257\n",
      "      B-MISC     0.1032    0.0488    0.0663       922\n",
      "      I-MISC     0.0000    0.0000    0.0000       346\n",
      "\n",
      "   micro avg     0.3901    0.3978    0.3939      8603\n",
      "   macro avg     0.3011    0.3194    0.2870      8603\n",
      "weighted avg     0.3921    0.3978    0.3706      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/train.txt\".format(language),model=bilstm_crf,max_length=max_length,train_or_valid=1)\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/validation.txt\".format(language),model=bilstm_crf,max_length=max_length,train_or_valid=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
