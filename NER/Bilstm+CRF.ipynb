{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# 在BiLSTM+CRF模型中，BiLSTM部分可以使用Pytorch等深度学习框架，CRF部分必须手写完成。\n",
    "# https://github.com/phipleg/keras/blob/crf/keras/layers/crf.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ChainCRF\n",
    "import importlib\n",
    "importlib.reload(model.ChainCRF)\n",
    "from model.ChainCRF import ChainCRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        # self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # 词嵌入层\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # 定义CRF层\n",
    "        self.crf = ChainCRF(num_classes)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence, mask,targets=None):\n",
    "        # 获取词嵌入\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        embeds = sentence\n",
    "        # BiLSTM层\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # [batch_size,max_length,hidden_dim]\n",
    "\n",
    "        # 线性映射到标签空间\n",
    "        # print(lstm_out.shape)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        # emissions = self.sigmoid(emissions)\n",
    "\n",
    "        # print(emissions.shape)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 计算CRF损失\n",
    "            # mask = (sentence != 0)  # 使用 0 填充的词的位置作为掩码\n",
    "            crf_loss = self.crf(emissions,targets,mask)\n",
    "            return crf_loss\n",
    "        else:\n",
    "            # 测试时，使用维特比解码\n",
    "            tags = self.crf.viterbi_decode(emissions,mask)\n",
    "            return tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n",
      "tensor([ 5.6719e-02,  1.3333e-01,  7.2690e-01, -4.6336e-01, -5.9334e-01,\n",
      "         7.1746e-01, -1.1795e-01,  2.1614e-01,  4.3036e-01, -6.7053e-01,\n",
      "         5.7480e-01,  2.6827e-01,  2.4659e-02,  1.6066e-01,  2.0400e-01,\n",
      "        -3.9246e-01, -6.3294e-01,  6.2915e-01, -7.6340e-01,  1.1581e+00,\n",
      "         3.6218e-01,  3.1932e-01, -6.5613e-01, -4.7797e-01,  2.9885e-01,\n",
      "         6.2435e-01, -4.6060e-01, -9.6276e-01,  1.2214e+00, -2.3152e-01,\n",
      "        -6.8889e-02,  6.3519e-01,  7.7546e-01,  3.3128e-01, -3.5220e-01,\n",
      "         7.4236e-01, -6.6703e-01,  3.2260e-01,  4.3490e-01, -6.0154e-01,\n",
      "        -4.2067e-01,  2.1991e-02,  1.6378e-01, -9.5682e-01, -6.4464e-01,\n",
      "        -9.4111e-02, -2.7105e-01, -2.3312e-01, -3.8453e-01, -1.2665e+00,\n",
      "        -1.8289e-01,  5.0432e-01, -5.4260e-02,  1.1872e+00, -4.7143e-01,\n",
      "        -2.6562e+00,  4.4917e-01,  6.7218e-01,  1.4074e+00,  1.9179e-03,\n",
      "         4.0658e-01,  1.4287e+00, -1.0631e+00, -2.1713e-01,  4.7800e-01,\n",
      "         1.3170e-01,  1.2494e+00,  7.2980e-01, -2.0880e-01,  2.5449e-01,\n",
      "         1.2297e-01,  3.4922e-01,  2.4051e-01, -8.1023e-01,  5.2047e-01,\n",
      "         6.8801e-01,  6.7784e-02, -2.2132e-01, -1.2176e-01, -1.6238e-01,\n",
      "         5.3189e-01, -3.2943e-01, -5.3818e-01, -2.2957e-01, -1.4103e+00,\n",
      "         1.6494e-02, -1.3494e-01, -3.6251e-02, -7.7385e-01, -3.5178e-01,\n",
      "        -1.1230e-01, -3.7400e-01,  5.5139e-01, -1.9572e-01, -8.7050e-02,\n",
      "        -3.1469e-01, -4.2257e-01, -3.5286e-02, -2.4911e-02,  6.2131e-01])\n",
      "14041\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from check import *\n",
    "embed_size = 100\n",
    "glove_path=\"glove/\"\n",
    "vocab = torchtext.vocab.GloVe(name=\"6B\",dim=embed_size,cache=glove_path)\n",
    "print(vocab['中'])\n",
    "print(vocab['me'])\n",
    "from get_data import *\n",
    "language ='English'\n",
    "train_data = get_train_data(language)\n",
    "def sent2word(sentence):\n",
    "    return [w for w, _ in sentence]\n",
    "def sent2label(sentence):\n",
    "    return [l for _, l in sentence]\n",
    "max_length = max([len(l) for l in train_data])\n",
    "max_length = max(max_length, 128)\n",
    "sorted_labels = sorted_labels_chn if language == 'Chinese' else sorted_labels_eng\n",
    "def label2index(label):\n",
    "    return sorted_labels.index(label)\n",
    "#对句子的word进行词嵌入，对tag转换为index，对句子按照max_length进行截断或者填充，并产生对应的mask，最后产生对应的dataset\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Assuming train_data is a list of sentences where each sentence is a list of tuples (word, label)\n",
    "# Example: [('This', 'O'), ('is', 'O'), ('a', 'O'), ('sentence', 'B')]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vocab, label2index, max_length):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.label2index = label2index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "\n",
    "        # Extract words and labels\n",
    "        words = [w for w, _ in sentence]\n",
    "        labels = [l for _, l in sentence]\n",
    "\n",
    "        # Convert words to embeddings\n",
    "        word_embeddings = [self.vocab[w] for w in words]\n",
    "\n",
    "        # Convert labels to indices\n",
    "        label_indices = [self.label2index(l) for l in labels]\n",
    "\n",
    "        # Pad or truncate to max_length\n",
    "        if len(word_embeddings) < self.max_length:\n",
    "            pad_length = self.max_length - len(word_embeddings)\n",
    "            word_embeddings = word_embeddings + [torch.zeros_like(word_embeddings[0])] * pad_length\n",
    "            label_indices = label_indices + [0] * pad_length  # Assuming 0 is the index for padding\n",
    "        else:\n",
    "            word_embeddings = word_embeddings[:self.max_length]\n",
    "            label_indices = label_indices[:self.max_length]\n",
    "\n",
    "        # Create a mask\n",
    "        mask = [1] * min(len(words),max_length) + [0] * max(0,(self.max_length - len(words)))\n",
    "\n",
    "        return {\n",
    "            'word_embeddings': torch.stack(word_embeddings),\n",
    "            'label_indices': torch.tensor(label_indices),\n",
    "            'mask': torch.tensor(mask)\n",
    "        }\n",
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(train_data, vocab, label2index, max_length)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, num_epochs, learning_rate, device):\n",
    "    # Move the model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=num_epochs * len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move inputs, labels, and mask to GPU\n",
    "            inputs = batch['word_embeddings'].to(device)\n",
    "            labels = batch['label_indices'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(inputs, mask, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.set_postfix_str(\"Epoch:{}, Loss:{}\".format(epoch + 1, sum_loss / len(train_loader)))\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, train_loader, num_epochs, and learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "bilstm_crf = BiLSTM_CRF(len(sorted_labels),embed_size,hidden_dim)\n",
    "file = \"BILSTM_CRF_{}.bin\".format(language)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "mode = False\n",
    "if mode:\n",
    "    train(bilstm_crf,dataloader,10,1e-2,device)\n",
    "    torch.save(bilstm_crf.state_dict(),file)\n",
    "else:\n",
    "    bilstm_crf.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycheck(language, vocab, res_file, model, max_length, train_or_valid,device):\n",
    "    valid = get_data_from_file(res_file)\n",
    "    pred_path = \"example_data/BILSTM_CRF_{}_{}.txt\".format(language, \"train\" if train_or_valid else \"dev\")\n",
    "    valid_data = CustomDataset(valid, vocab, label2index, max_length)\n",
    "    valdataloader = DataLoader(valid_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Move the model to GPU\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    with open(pred_path, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            iter = 0\n",
    "            for val in valdataloader:\n",
    "                # Move inputs and masks to GPU\n",
    "                word_embeddings = val['word_embeddings'].to(device)\n",
    "                masks = val['mask'].to(device)\n",
    "\n",
    "                preds = model(word_embeddings, masks)\n",
    "\n",
    "                for pred, mask in zip(preds, masks):\n",
    "                    pred_labels = []\n",
    "                    for i in range(len(pred)):\n",
    "                        if mask[i] == 1:\n",
    "                            f.write(valid[iter][i][0] + \" \" + sorted_labels[pred[i]] + '\\n')\n",
    "                            pred_labels.append(sorted_labels[pred[i]])\n",
    "                        else:\n",
    "                            f.write('\\n')\n",
    "                            iter = iter + 1\n",
    "                            break\n",
    "\n",
    "    # Move the model back to CPU if needed\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    check(language, \"{}/{}.txt\".format(language, \"train\" if train_or_valid else \"validation\"), pred_path)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, vocab, res_file, max_length, train_or_valid, label2index, sorted_labels\n",
    "# Make sure to replace the imports with the actual modules and functions in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mycheck(language\u001b[39m=\u001b[39;49mlanguage,vocab\u001b[39m=\u001b[39;49mvocab,res_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/train.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(language),model\u001b[39m=\u001b[39;49mbilstm_crf,max_length\u001b[39m=\u001b[39;49mmax_length,train_or_valid\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      2\u001b[0m mycheck(language\u001b[39m=\u001b[39mlanguage,vocab\u001b[39m=\u001b[39mvocab,res_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/validation.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(language),model\u001b[39m=\u001b[39mbilstm_crf,max_length\u001b[39m=\u001b[39mmax_length,train_or_valid\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,device\u001b[39m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mmycheck\u001b[0;34m(language, vocab, res_file, model, max_length, train_or_valid, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m word_embeddings \u001b[39m=\u001b[39m val[\u001b[39m'\u001b[39m\u001b[39mword_embeddings\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m masks \u001b[39m=\u001b[39m val[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m preds \u001b[39m=\u001b[39m model(word_embeddings, masks)\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m pred, mask \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(preds, masks):\n\u001b[1;32m     22\u001b[0m     pred_labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mBiLSTM_CRF.forward\u001b[0;34m(self, sentence, mask, targets)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m crf_loss\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39m# 测试时，使用维特比解码\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     tags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrf\u001b[39m.\u001b[39;49mviterbi_decode(emissions,mask)\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m tags\n",
      "File \u001b[0;32m~/Desktop/干活/人工智能A/pj2/NER/NER/model/ChainCRF.py:162\u001b[0m, in \u001b[0;36mChainCRF.viterbi_decode\u001b[0;34m(self, emission, mask)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode the highest scoring sequence of tags using the Viterbi algorithm.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m:param emission: [batch_size, seq_len, num_classes] - emission scores\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m:param mask: [batch_size, seq_len] - binary mask for sequence lengths\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: [batch_size, seq_len] - the tag indices of the highest scoring sequence\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m batch_size, seq_len, _ \u001b[39m=\u001b[39m emission\u001b[39m.\u001b[39msize()\n\u001b[0;32m--> 162\u001b[0m score, path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviterbi2(emission, mask)\n\u001b[1;32m    163\u001b[0m best_path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((batch_size, seq_len), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m    164\u001b[0m _, best_last_tag \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(score[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/干活/人工智能A/pj2/NER/NER/model/ChainCRF.py:139\u001b[0m, in \u001b[0;36mChainCRF.viterbi2\u001b[0;34m(self, emission, mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m temp_emission \u001b[39m=\u001b[39m emission[:, j, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[39m# 32 ,1,9\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# print(\"prev\", prev)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m# print(\"temp_emission:\", temp_emission)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m last_score \u001b[39m=\u001b[39m prev \u001b[39m+\u001b[39;49m temp_emission\n\u001b[1;32m    140\u001b[0m \u001b[39m# 32 ,9, 9\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# print(\"last_score:\", last_score)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m# print(\"sigmoidU[ll.int(), :].unsqueeze(0):\",\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m#   sigmoidU[ll.int(), :].unsqueeze(0))\u001b[39;00m\n\u001b[1;32m    144\u001b[0m sigmoidU_ \u001b[39m=\u001b[39m sigmoidU\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m last_score\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/train.txt\".format(language),model=bilstm_crf,max_length=max_length,train_or_valid=1,device=device)\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/validation.txt\".format(language),model=bilstm_crf,max_length=max_length,train_or_valid=0,device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
