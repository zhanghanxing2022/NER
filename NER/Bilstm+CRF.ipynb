{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.ChainCRF\n",
    "import importlib\n",
    "import dataLoad.lload\n",
    "importlib.reload(model.ChainCRF)\n",
    "importlib.reload(dataLoad.lload)\n",
    "from  dataLoad.lload import CustomDataset\n",
    "from  dataLoad.lload import *\n",
    "import numpy as np\n",
    "from model.ChainCRF import BLITM,ChainCRF\n",
    "import torchtext\n",
    "from get_data import *\n",
    "from check import *\n",
    "from collections import Counter,defaultdict\n",
    "embed_size = 100\n",
    "language = \"English\"\n",
    "min_freq = 4 if language == \"Chinese\" else 2\n",
    "##Chinese 300 100 10,1e-3  Loss:281.3659775416056 micro avg  0.9368 \n",
    "##Chinese 100 100 10,1e-3  Loss:281.3659775416056 micro avg  0.9447 \n",
    "##Chinese 50 100 10,1e-4  Loss:281.3659775416056 micro avg 0.6839 \n",
    "##Chinese 50 100 10,5e-4  Loss:277.4861424763997 micro avg 0.9065\n",
    "## 0.7683 \n",
    "## English\n",
    "##  pretrain 8317 +0.8276 =0.8320  1e-3 10\n",
    "##  pretrain 8317 +0.8276 =0.8347  1e-3 10 zeros\n",
    "##  pretrain 8317 +0.8276 =0.8399  1e-4 10 zeros\n",
    "#   0.8381 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_length,embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.bilstm = BLITM(num_classes,vocab_length,embedding_dim,hidden_dim)\n",
    "        self.crf = ChainCRF(num_classes)\n",
    "\n",
    "    def forward(self, sentence, mask, targets=None, pre_train=None):\n",
    "    \n",
    "        emissions = self.bilstm(sentence)\n",
    "        if targets is not None:\n",
    "            # 计算CRF损失\n",
    "            # mask = (sentence != 0)  # 使用 0 填充的词的位置作为掩码\n",
    "            crf_loss = self.crf(emissions, targets, mask)\n",
    "            return crf_loss\n",
    "        else:\n",
    "            # 测试时，使用维特比解码\n",
    "            tags = self.crf.viterbi_decode(emissions, mask)\n",
    "            return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123967\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = get_train_data(language)\n",
    "import os\n",
    "import torchtext\n",
    "train_word = [word  for sentence in train_data for word,label in sentence]\n",
    "print(len(train_word))\n",
    "vocab = torchtext.vocab.vocab(Counter(train_word),min_freq=min_freq,specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "print(vocab.get_default_index())\n",
    "def sent2word(sentence):\n",
    "    return [w for w, _ in sentence]\n",
    "def sent2label(sentence):\n",
    "    return [l for _, l in sentence]\n",
    "max_length = max([len(l) for l in train_data])\n",
    "max_length = max(max_length, 256)\n",
    "sorted_labels = sorted_labels_chn if language == 'Chinese' else sorted_labels_eng\n",
    "def label2index(label):\n",
    "    return sorted_labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_embeddings': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]),\n",
       " 'label_indices': tensor([ 0,  0, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 13, 14, 14, 14, 14, 15,\n",
       "          0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'max_length': 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(train_data, vocab, label2index, max_length)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, num_epochs, learning_rate, device):\n",
    "    # Move the model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=num_epochs * len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move inputs, labels, and mask to GPU\n",
    "            length = batch['max_length']\n",
    "            max_length = np.argmax(length)\n",
    "            aaa = length[max_length]\n",
    "            inputs = batch['word_embeddings'][:,:aaa].to(device)\n",
    "            labels = batch['label_indices'][:,:aaa].to(device)\n",
    "            mask =  batch['mask'].to(device)[:,:aaa].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(inputs, mask, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.set_postfix_str(\"Epoch:{}, Loss:{}\".format(epoch + 1, sum_loss / len(train_loader)))\n",
    "        print(\" \")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, train_loader, num_epochs, and learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 121/1200 [00:18<01:56,  9.29it/s, Epoch:1, Loss:329.22883129119873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 241/1200 [00:36<02:05,  7.63it/s, Epoch:2, Loss:296.39825750986734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 361/1200 [00:54<01:46,  7.84it/s, Epoch:3, Loss:279.52405109405515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 481/1200 [01:11<01:40,  7.13it/s, Epoch:4, Loss:280.946046257019]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 601/1200 [01:30<01:05,  9.15it/s, Epoch:5, Loss:281.3986678441366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 720/1200 [01:48<01:20,  5.94it/s, Epoch:6, Loss:280.7885049819946]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 841/1200 [02:06<00:42,  8.43it/s, Epoch:7, Loss:282.91113929748536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 961/1200 [02:25<00:25,  9.55it/s, Epoch:8, Loss:279.2567253748576] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1081/1200 [02:43<00:16,  7.33it/s, Epoch:9, Loss:283.78191502888996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:01<00:00,  6.62it/s, Epoch:10, Loss:281.18672478993733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "bilstm_crf = BiLSTM_CRF(len(sorted_labels),len(vocab),embed_size,hidden_dim)\n",
    "pretrain_file = \"./bilstm_crf/pretrain/BILSTM_{}.bin\".format(language)\n",
    "file = \"BILSTM_CRF_{}.bin\".format(language)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# bilstm_crf.bilstm.load_state_dict(torch.load(pretrain_file))\n",
    "mode = True\n",
    "if mode:\n",
    "    train(bilstm_crf,dataloader,10,1e-3,device)\n",
    "    torch.save(bilstm_crf.state_dict(),file)\n",
    "else:\n",
    "    bilstm_crf.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycheck(language, vocab, res_file, model, train_or_valid,device):\n",
    "    valid = get_data_from_file(res_file)\n",
    "    pred_path = \"example_data/BILSTM_CRF_{}_{}.txt\".format(language, \"train\" if train_or_valid else \"dev\")\n",
    "    valid_data = CustomDataset(valid, vocab, label2index, 256)\n",
    "    valdataloader = DataLoader(valid_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Move the model to GPU\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    with open(pred_path, \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            iter = 0\n",
    "            for batch in valdataloader:\n",
    "                # Move inputs, labels, and mask to GPU\n",
    "                length = batch['max_length']\n",
    "                max_length = np.argmax(length)\n",
    "                aaa = length[max_length]\n",
    "                word_embeddings = batch['word_embeddings'][:,:aaa]\n",
    "                masks= batch['mask'][:,:aaa]\n",
    "\n",
    "                preds = model(word_embeddings, masks)\n",
    "\n",
    "                for pred in preds:\n",
    "                    pred_labels = []\n",
    "                    for i in range(len(valid[iter])):\n",
    "                        f.write(valid[iter][i][0] + \" \" + sorted_labels[pred[i]] + '\\n')\n",
    "                        pred_labels.append(sorted_labels[pred[i]])\n",
    "                    f.write('\\n')\n",
    "                    iter = iter + 1\n",
    "\n",
    "    # Move the model back to CPU if needed\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    check(language, \"{}/{}.txt\".format(language, \"train\" if train_or_valid else \"validation\"), pred_path)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, vocab, res_file, max_length, train_or_valid, label2index, sorted_labels\n",
    "# Make sure to replace the imports with the actual modules and functions in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-NAME     0.9942    0.9907    0.9924       861\n",
      "      M-NAME     0.9946    0.9905    0.9926       740\n",
      "      E-NAME     0.9942    0.9907    0.9924       861\n",
      "      S-NAME     0.9674    0.9780    0.9727        91\n",
      "      B-CONT     0.9961    0.9923    0.9942       260\n",
      "      M-CONT     0.9959    0.9800    0.9879       499\n",
      "      E-CONT     0.9846    0.9808    0.9827       260\n",
      "      S-CONT     0.0000    0.0000    0.0000         0\n",
      "       B-EDU     0.9919    0.9930    0.9924       858\n",
      "       M-EDU     0.9902    0.9915    0.9909      1536\n",
      "       E-EDU     0.9929    0.9837    0.9883       858\n",
      "       S-EDU     0.0000    0.0000    0.0000         0\n",
      "     B-TITLE     0.9866    0.9840    0.9853      6296\n",
      "     M-TITLE     0.9857    0.9881    0.9869     14813\n",
      "     E-TITLE     0.9960    0.9933    0.9947      6296\n",
      "     S-TITLE     0.0000    0.0000    0.0000         0\n",
      "       B-ORG     0.9934    0.9872    0.9903      4603\n",
      "       M-ORG     0.9946    0.9952    0.9949     33762\n",
      "       E-ORG     0.9838    0.9781    0.9809      4603\n",
      "       S-ORG     0.0000    0.0000    0.0000         1\n",
      "      B-RACE     0.9820    0.9732    0.9776       112\n",
      "      M-RACE     1.0000    0.3333    0.5000         6\n",
      "      E-RACE     0.9823    0.9911    0.9867       112\n",
      "      S-RACE     0.0000    0.0000    0.0000         3\n",
      "       B-PRO     0.9860    0.9791    0.9825       287\n",
      "       M-PRO     0.9806    0.9865    0.9835       666\n",
      "       E-PRO     0.9756    0.9756    0.9756       287\n",
      "       S-PRO     0.0000    0.0000    0.0000         0\n",
      "       B-LOC     0.9167    0.9362    0.9263        47\n",
      "       M-LOC     0.8987    0.9930    0.9435       143\n",
      "       E-LOC     0.9512    0.8298    0.8864        47\n",
      "       S-LOC     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.9910    0.9903    0.9907     78908\n",
      "   macro avg     0.7661    0.7436    0.7494     78908\n",
      "weighted avg     0.9910    0.9903    0.9906     78908\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-NAME     0.9703    0.9608    0.9655       102\n",
      "      M-NAME     0.9595    0.9467    0.9530        75\n",
      "      E-NAME     0.9700    0.9510    0.9604       102\n",
      "      S-NAME     1.0000    0.8750    0.9333         8\n",
      "      B-CONT     1.0000    0.9697    0.9846        33\n",
      "      M-CONT     1.0000    0.9688    0.9841        64\n",
      "      E-CONT     1.0000    1.0000    1.0000        33\n",
      "      S-CONT     0.0000    0.0000    0.0000         0\n",
      "       B-EDU     0.9810    0.9717    0.9763       106\n",
      "       M-EDU     0.9833    1.0000    0.9916       177\n",
      "       E-EDU     0.9905    0.9811    0.9858       106\n",
      "       S-EDU     0.0000    0.0000    0.0000         0\n",
      "     B-TITLE     0.9188    0.9028    0.9107       689\n",
      "     M-TITLE     0.8954    0.9263    0.9106      1479\n",
      "     E-TITLE     0.9898    0.9840    0.9869       689\n",
      "     S-TITLE     0.0000    0.0000    0.0000         0\n",
      "       B-ORG     0.9606    0.9349    0.9476       522\n",
      "       M-ORG     0.9552    0.9542    0.9547      3622\n",
      "       E-ORG     0.9067    0.8755    0.8908       522\n",
      "       S-ORG     0.0000    0.0000    0.0000         0\n",
      "      B-RACE     1.0000    1.0000    1.0000        14\n",
      "      M-RACE     0.0000    0.0000    0.0000         0\n",
      "      E-RACE     1.0000    1.0000    1.0000        14\n",
      "      S-RACE     0.0000    0.0000    0.0000         1\n",
      "       B-PRO     0.8571    1.0000    0.9231        18\n",
      "       M-PRO     0.8250    1.0000    0.9041        33\n",
      "       E-PRO     0.9000    1.0000    0.9474        18\n",
      "       S-PRO     0.0000    0.0000    0.0000         0\n",
      "       B-LOC     0.5000    1.0000    0.6667         2\n",
      "       M-LOC     0.7500    1.0000    0.8571         6\n",
      "       E-LOC     1.0000    1.0000    1.0000         2\n",
      "       S-LOC     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.9429    0.9437    0.9433      8437\n",
      "   macro avg     0.6973    0.7251    0.7073      8437\n",
      "weighted avg     0.9434    0.9437    0.9434      8437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/train.txt\".format(language),model=bilstm_crf,train_or_valid=1,device=device)\n",
    "mycheck(language=language,vocab=vocab,res_file=\"{}/validation.txt\".format(language),model=bilstm_crf,train_or_valid=0,device=device)\n",
    "# loss:114 0.3803  0.3954\n",
    "# -92.54755198711143 0.3460 0.3643"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
